{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing kubeflow pipelines client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,json, os\n",
    "job_class = os.getenv(\"DKUBE_JOB_CLASS\")\n",
    "if not job_class:\n",
    "    !{sys.executable} -m pip install kfp==1.4.0 kfp-server-api==1.2.0 --upgrade >> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLURM_CLUSTER=\"slurm111\"\n",
    "run_id = 0\n",
    "project_id = os.environ.get(\"DKUBE_PROJECT_ID\", \"\")\n",
    "project_name = os.environ.get(\"DKUBE_PROJECT_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check DKube project settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project clinical-reg exists.. reusing\n"
     ]
    }
   ],
   "source": [
    "if not project_id:\n",
    "    project_name = \"clinical-reg\"\n",
    "    from dkube.sdk import *\n",
    "    api = DkubeApi(token=os.getenv(\"DKUBE_USER_ACCESS_TOKEN\"))\n",
    "    try:\n",
    "        project = DkubeProject(project_name)\n",
    "        res = api.create_project(project)\n",
    "        project_id = res[\"id\"]\n",
    "    except Exception as e:\n",
    "        if e.reason.lower()==\"conflict\":\n",
    "            project_id = api.get_project_id(project_name)\n",
    "            print(f\"Project {project_name} exists.. reusing\")\n",
    "        else:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project name = clinical-reg\n",
      "project id = hpjajn\n"
     ]
    }
   ],
   "source": [
    "print(f\"project name = {project_name}\")\n",
    "print(f\"project id = {project_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import kfp pkgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "from kubernetes import client as k8s_client\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_component = '''\n",
    "name: create_dkube_resource\n",
    "description: |\n",
    "    creates dkube resources required for pipeline.\n",
    "metadata:\n",
    "  annotations: {platform: 'Dkube'}\n",
    "  labels: {stage: 'create_dkube_resource', logger: 'dkubepl', wfid: '{{workflow.uid}}', runid: '{{pod.name}}'}\n",
    "inputs:\n",
    "  - {name: token,      type: String,   optional: false,\n",
    "    description: 'Required. Dkube authentication token.'}\n",
    "  - {name: user,      type: String,   optional: false,\n",
    "    description: 'Required. Dkube Logged in User name.'}\n",
    "  - {name: project_id,      type: String,   optional: false,\n",
    "    description: 'Required. Dkube Project name.'}\n",
    "implementation:\n",
    "  container:\n",
    "    image: docker.io/ocdr/dkube-examples-setup:cli-reg\n",
    "    command: ['python3', 'regressionsetup.py']\n",
    "    args: [\n",
    "      --auth_token, {inputValue: token},\n",
    "      --user, {inputValue: user},\n",
    "      --project_id, {inputValue: project_id}\n",
    "    ]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define e2e regression Pipeline with Dkube components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp import components\n",
    "from kfp.components._yaml_utils import load_yaml\n",
    "from kfp.components._yaml_utils import dump_yaml\n",
    "from kubernetes import client as k8s_client\n",
    "\n",
    "import os\n",
    "import json\n",
    "from random import randint\n",
    "\n",
    "from dkube.slurm.job import *\n",
    "from dkube.sdk import DkubeTraining, DkubePreprocessing\n",
    "from dkube.slurm.job_properties import JobProperties\n",
    "\n",
    "def _component(stage, name):\n",
    "    with open('/mnt/dkube/pipeline/components/{}/component.yaml'.format(stage), 'rb') as stream:\n",
    "        cdict = load_yaml(stream)\n",
    "        cdict['name'] = name\n",
    "        cyaml = dump_yaml(cdict)\n",
    "        return components.load_component_from_text(cyaml)\n",
    "        \n",
    "setup_op = kfp.components.load_component(text = setup_component)\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='dkube-regression-pl',\n",
    "    description='sample regression pipeline with dkube components'\n",
    ")\n",
    "\n",
    "def d3pipeline(\n",
    "    user,\n",
    "    token,\n",
    "    project_id=project_id,\n",
    "    tags = json.dumps([]),\n",
    "    #Slurm cluster name\n",
    "    slurm_cluster = SLURM_CLUSTER,\n",
    "    #Slurm cluster properties\n",
    "    slurm_jobprops = JobProperties().to_dict(),\n",
    "    \n",
    "    \n",
    "    #Clinical preprocess\n",
    "    clinical_preprocess_script=\"python cli-pre-processing.py\",\n",
    "    clinical_preprocess_datasets=json.dumps([\"clinical\"]),\n",
    "    clinical_preprocess_input_mounts=json.dumps([\"/opt/dkube/input\"]),\n",
    "    clinical_preprocess_outputs=json.dumps([\"clinical-preprocessed\"]),\n",
    "    clinical_preprocess_output_mounts=json.dumps([\"/opt/dkube/output\"]),\n",
    "    \n",
    "    #Image preprocess\n",
    "    image_preprocess_script=\"python img-pre-processing.py\",\n",
    "    image_preprocess_datasets=json.dumps([\"images\"]),\n",
    "    image_preprocess_input_mounts=json.dumps([\"/opt/dkube/input\"]),\n",
    "    image_preprocess_outputs=json.dumps([\"images-preprocessed\"]),\n",
    "    image_preprocess_output_mounts=json.dumps([\"/opt/dkube/output\"]),\n",
    "    \n",
    "    #Clinical split\n",
    "    clinical_split_script=\"python split.py --datatype clinical\",\n",
    "    clinical_split_datasets=json.dumps([\"clinical-preprocessed\"]),\n",
    "    clinical_split_input_mounts=json.dumps([\"/opt/dkube/input\"]),\n",
    "    clinical_split_outputs=json.dumps([\"clinical-train\", \"clinical-test\", \"clinical-val\"]),\n",
    "    clinical_split_output_mounts=json.dumps([\"/opt/dkube/outputs/train\", \"/opt/dkube/outputs/test\", \"/opt/dkube/outputs/val\"]),\n",
    "    \n",
    "    #Image split\n",
    "    image_split_script=\"python split.py --datatype image\",\n",
    "    image_split_datasets=json.dumps([\"images-preprocessed\"]),\n",
    "    image_split_input_mounts=json.dumps([\"/opt/dkube/input\"]),\n",
    "    image_split_outputs=json.dumps([\"images-train\", \"images-test\", \"images-val\"]),\n",
    "    image_split_output_mounts=json.dumps([\"/opt/dkube/outputs/train\", \"/opt/dkube/outputs/test\", \"/opt/dkube/outputs/val\"])\t,\n",
    "    \n",
    "    #RNA split\n",
    "    rna_split_script=\"python split.py --datatype rna\",\n",
    "    rna_split_datasets=json.dumps([\"rna\"]),\n",
    "    rna_split_input_mounts=json.dumps([\"/opt/dkube/input\"]),\n",
    "    rna_split_outputs=json.dumps([\"rna-train\", \"rna-test\", \"rna-val\"]),\n",
    "    rna_split_output_mounts=json.dumps([\"/opt/dkube/outputs/train\", \"/opt/dkube/outputs/test\", \"/opt/dkube/outputs/val\"]),\n",
    "    \n",
    "    #Training\n",
    "    job_group = 'default',\n",
    "    #Framework. One of tensorflow, pytorch, sklearn\n",
    "    framework = \"tensorflow\",\n",
    "    #Framework version\n",
    "    version = \"2.3.0\",\n",
    "    #In notebook DKUBE_USER_ACCESS_TOKEN is automatically picked up from env variable\n",
    "    #Or any other custom image name can be supplied.\n",
    "    #For custom private images, please input username/password\n",
    "    training_container=json.dumps({'image':'ocdr/dkube-datascience-tf-cpu:v2.3.0-6'}),\n",
    "    #Name of the workspace in dkube. Update accordingly if different name is used while creating a workspace in dkube.\n",
    "    training_program=\"regression\",\n",
    "    #Script to run inside the training container    \n",
    "    training_script=\"python train_nn.py --epochs 5\",\n",
    "    #Input datasets for training. Update accordingly if different name is used while creating dataset in dkube.    \n",
    "    training_datasets=json.dumps([\"clinical-train\", \"clinical-val\", \"images-train\",\n",
    "                                  \"images-val\", \"rna-train\", \"rna-val\"]),\n",
    "    training_input_dataset_mounts=json.dumps([\"/opt/dkube/inputs/train/clinical\", \"/opt/dkube/inputs/val/clinical\",\n",
    "                                      \"/opt/dkube/inputs/train/images\", \"/opt/dkube/inputs/val/images\",\n",
    "                                      \"/opt/dkube/inputs/train/rna\", \"/opt/dkube/inputs/val/rna\"]),\n",
    "    training_outputs=json.dumps([\"regression-model\"]),\n",
    "    training_output_mounts=json.dumps([\"/opt/dkube/output\"]),\n",
    "    #Request gpus as needed. Val 0 means no gpu, then training_container=docker.io/ocdr/dkube-datascience-tf-cpu:v1.12    \n",
    "    training_gpus=0,\n",
    "    #Any envs to be passed to the training program    \n",
    "    training_envs=json.dumps([{\"steps\": 100}]),\n",
    "    \n",
    "    tuning=json.dumps({}),\n",
    "    \n",
    "    #Evaluation\n",
    "    evaluation_script=\"python evaluate.py\",\n",
    "    evaluation_datasets=json.dumps([\"clinical-test\", \"images-test\", \"rna-test\"]),\n",
    "    evaluation_input_dataset_mounts=json.dumps([\"/opt/dkube/inputs/test/clinical\", \"/opt/dkube/inputs/test/images\",\n",
    "                                      \"/opt/dkube/inputs/test/rna\"]),\n",
    "    evaluation_models=json.dumps([\"regression-model\"]),\n",
    "    evaluation_input_model_mounts=json.dumps([\"/opt/dkube/inputs/model\"]),\n",
    "    \n",
    "    #Serving\n",
    "    #Device to be used for serving - dkube mnist example trained on gpu needs gpu for serving else set this param to 'cpu'\n",
    "    serving_device='cpu',\n",
    "    #Serving image\n",
    "    serving_image=json.dumps({'image':'ocdr/tensorflowserver:2.3.0'}),\n",
    "    #Transformer image\n",
    "    transformer_image=json.dumps({'image':'ocdr/dkube-datascience-tf-cpu:v2.3.0-17'}),\n",
    "    #Script to execute the transformer\n",
    "    transformer_code=\"clinical_reg/transformer.py\"):\n",
    "    \n",
    "    create_resource = setup_op(user = user, token = token, project_id = project_id).set_display_name(\"(dkube)create_resources\")\n",
    "    create_resource.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    \n",
    "    \n",
    "    clinical_preprocess_job = DkubePreprocessing(str(user), tags=tags)\n",
    "    clinical_preprocess_job.update_container(image_url=\"ocdr/dkube-datascience-tf-cpu:v2.3.0-6\")\n",
    "    clinical_preprocess_job.update_startupscript(str(clinical_preprocess_script))\n",
    "    clinical_preprocess_job.add_code(str(training_program))\n",
    "    clinical_preprocess_job.add_input_dataset(\"clinical\", mountpath=\"/opt/dkube/input\")\n",
    "    clinical_preprocess_job.add_output_dataset(\"clinical-preprocessed\", mountpath=\"/opt/dkube/output\")\n",
    "    clinical_preprocess = dkube_slurmjob_preprocessing_op(\n",
    "                        slurm_cluster,\n",
    "                        slurm_jobprops,\n",
    "                        str(user), \n",
    "                        str(token), \n",
    "                        clinical_preprocess_job.job.to_dict()).set_display_name(\"(slurm)clinical-preprocess\").after(create_resource)\n",
    "    \n",
    "    \n",
    "    image_preprocess_job = DkubePreprocessing(str(user), tags=tags)\n",
    "    image_preprocess_job.update_container(image_url=\"ocdr/dkube-datascience-tf-cpu:v2.3.0-6\")\n",
    "    image_preprocess_job.update_startupscript(str(image_preprocess_script))\n",
    "    image_preprocess_job.add_code(str(training_program))\n",
    "    image_preprocess_job.add_input_dataset(\"images\", mountpath=\"/opt/dkube/input\")\n",
    "    image_preprocess_job.add_output_dataset(\"images-preprocessed\", mountpath=\"/opt/dkube/output\")\n",
    "    \n",
    "    image_preprocess = dkube_slurmjob_preprocessing_op(\n",
    "                        slurm_cluster,\n",
    "                        slurm_jobprops,\n",
    "                        str(user), \n",
    "                        str(token), \n",
    "                        image_preprocess_job.job.to_dict()).set_display_name(\"(slurm)images-preprocess\").after(create_resource)\n",
    "    \n",
    "    clinical_split_job = DkubePreprocessing(str(user), tags=tags)\n",
    "    clinical_split_job.update_container(image_url=\"ocdr/dkube-datascience-tf-cpu:v2.3.0-6\")\n",
    "    clinical_split_job.update_startupscript(str(clinical_split_script))\n",
    "    clinical_split_job.add_code(str(training_program))\n",
    "    clinical_split_job.add_input_dataset(\"clinical-preprocessed\", mountpath=\"/opt/dkube/input\")\n",
    "    clinical_split_job.add_output_dataset(\"clinical-train\", mountpath=\"/opt/dkube/outputs/train\")\n",
    "    clinical_split_job.add_output_dataset(\"clinical-test\", mountpath=\"/opt/dkube/outputs/test\")\n",
    "    clinical_split_job.add_output_dataset(\"clinical-val\", mountpath=\"/opt/dkube/outputs/val\")\n",
    "    \n",
    "    clinical_split = dkube_slurmjob_preprocessing_op(\n",
    "                    slurm_cluster,\n",
    "                    slurm_jobprops,\n",
    "                    str(user), \n",
    "                    str(token), \n",
    "                    clinical_split_job.job.to_dict()).set_display_name(\"(slurm)clinical-split\").after(clinical_preprocess)\n",
    "    \n",
    "    \n",
    "\n",
    "    image_split_job = DkubePreprocessing(str(user), tags=tags)\n",
    "    image_split_job.update_container(image_url=\"ocdr/dkube-datascience-tf-cpu:v2.3.0-6\")\n",
    "    image_split_job.update_startupscript(str(image_split_script))\n",
    "    image_split_job.add_code(str(training_program))\n",
    "    image_split_job.add_input_dataset(\"images-preprocessed\", mountpath=\"/opt/dkube/input\")\n",
    "    image_split_job.add_output_dataset(\"images-train\", mountpath=\"/opt/dkube/outputs/train\")\n",
    "    image_split_job.add_output_dataset(\"images-test\", mountpath=\"/opt/dkube/outputs/test\")\n",
    "    image_split_job.add_output_dataset(\"images-val\", mountpath=\"/opt/dkube/outputs/val\")\n",
    "    \n",
    "    image_split = dkube_slurmjob_preprocessing_op(\n",
    "                    slurm_cluster,\n",
    "                    slurm_jobprops,\n",
    "                    str(user), \n",
    "                    str(token), \n",
    "                    image_split_job.job.to_dict()).set_display_name(\"(slurm)images-split\").after(image_preprocess)\n",
    "    \n",
    "                                      \n",
    "    rna_split  = _component('preprocess', 'rna-split')(container=training_container, tags=[tags],\n",
    "                                      program=training_program, run_script=rna_split_script,\n",
    "                                      datasets=rna_split_datasets, outputs=rna_split_outputs,\n",
    "                                      input_dataset_mounts=rna_split_input_mounts, output_mounts=rna_split_output_mounts).after(create_resource)\n",
    "\n",
    "    training_job = DkubeTraining(str(user), tags=tags)\n",
    "    training_job.update_container(image_url=\"ocdr/dkube-datascience-tf-cpu:v2.3.0-6\")\n",
    "    training_job.update_startupscript(str(training_script))\n",
    "    training_job.add_code(str(training_program))\n",
    "    training_job.add_input_dataset(\"clinical-train\", mountpath='/opt/dkube/inputs/train/clinical')\n",
    "    training_job.add_input_dataset(\"clinical-val\", mountpath='/opt/dkube/inputs/val/clinical')\n",
    "    training_job.add_input_dataset(\"images-train\", mountpath='/opt/dkube/inputs/train/images')\n",
    "    training_job.add_input_dataset(\"images-val\", mountpath='/opt/dkube/inputs/val/images')\n",
    "    training_job.add_input_dataset(\"rna-train\", mountpath='/opt/dkube/inputs/train/rna')\n",
    "    training_job.add_input_dataset(\"rna-val\", mountpath='/opt/dkube/inputs/val/rna')\n",
    "    training_job.add_output_model(\"regression-model\", mountpath='/opt/dkube/output')\n",
    "    training_job.add_envvars({\"steps\": \"100\"})\n",
    "    \n",
    "    \n",
    "    train = dkube_slurmjob_op(\n",
    "                slurm_cluster,\n",
    "                slurm_jobprops,\n",
    "                str(user), \n",
    "                str(token), \n",
    "                training_job.job.to_dict()).set_display_name(\"(slurm) training\").after(clinical_split).after(image_split).after(rna_split)\n",
    "    \n",
    "\n",
    "    serving     = _component('serving', 'model-serving')(model=\"regression-model\", device=serving_device,\n",
    "                                serving_image=serving_image,\n",
    "                                transformer_image=transformer_image,\n",
    "                                transformer_project=training_program,\n",
    "                                transformer_code=transformer_code).set_display_name(\"(dkube) serving\").after(train)\n",
    "    inference   = _component('viewer', 'model-inference')(servingurl=serving.outputs['servingurl'],\n",
    "                                 servingexample='regression', viewtype='inference').set_display_name(\"(dkube) viewer\").after(serving)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and generate tar ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, string\n",
    "suffix = ''.join(random.choice(string.ascii_uppercase + string.ascii_lowercase + string.digits) for _ in range(6))\n",
    "pipeline_filename = 'dkube_regression_pl_full.tar.gz'\n",
    "pipeline_name = 'Regression Pipeline' + \"-\" + suffix\n",
    "compiler.Compiler().compile(d3pipeline, pipeline_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=/pipeline/#/pipelines/details/2bc166d1-348b-47f8-ac94-cb4fa47dca5f>Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "existing_token = os.getenv(\"DKUBE_USER_ACCESS_TOKEN\")\n",
    "client = kfp.Client(host=os.getenv(\"KF_PIPELINES_ENDPOINT\"), existing_token=existing_token, namespace=os.getenv(\"USERNAME\"))\n",
    "try:\n",
    "  client.upload_pipeline(pipeline_package_path = pipeline_filename, pipeline_name = pipeline_name, description = None)\n",
    "except BaseException as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hpjajn\n"
     ]
    }
   ],
   "source": [
    "if project_id:\n",
    "    print(project_id)\n",
    "    tags = f\"project:{project_id}\"\n",
    "    run_name = f\"[{project_name}] Clinical Reg {run_id}\"\n",
    "    experiment_name = f\"[{project_name}] experiment\"\n",
    "else:\n",
    "    print(project_id)\n",
    "    tags = []\n",
    "    run_name = f\"Clinical Reg {run_id}\"\n",
    "    experiment_name = \"Dkube - Clinical Reg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create regression experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/8c5f80a0-524d-4029-8d36-4318fabf68b7\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user = os.getenv(\"USER\")\n",
    "client.list_experiments(namespace=user)\n",
    "# Create a new experiment\n",
    "try:\n",
    "    clinical_experiment = client.create_experiment(name=experiment_name, namespace=user)\n",
    "except BaseException as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/22c285fb-40fb-46c8-9245-4f580d970a19\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user = os.getenv(\"USER\")\n",
    "auth_token = existing_token\n",
    "\n",
    "try:\n",
    "    run = client.run_pipeline(clinical_experiment.id, run_name, pipeline_package_path=pipeline_filename, service_account=user,\n",
    "                              params={\"user\":user, \"token\":auth_token, \"tags\": tags})\n",
    "except BaseException as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
